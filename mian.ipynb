{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985308c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30fded16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ea0186",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from prophet import Prophet\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error\n",
    "from collections import Counter\n",
    "import xgboost as xgb\n",
    "\n",
    "# 1. Load & initial clean\n",
    "df = pd.read_csv(\"totalload_new.csv\", engine=\"python\")\n",
    "\n",
    "# 1.1 Drop entirely empty columns\n",
    "df = df.dropna(axis=1, how=\"all\")\n",
    "\n",
    "# 1.2 Parse and rename timestamp\n",
    "df['ds'] = pd.to_datetime(df['BASE_DT'], format='%d-%m-%y %H:%M', errors='coerce')\n",
    "df = df.dropna(subset=['ds']).sort_values('ds').reset_index(drop=True)\n",
    "\n",
    "# 1.3 Impute numeric columns (including TOTAL_LOAD and weather)\n",
    "numeric_cols = df.select_dtypes(include=['int64','float64']).columns\n",
    "df[numeric_cols] = SimpleImputer(strategy=\"mean\").fit_transform(df[numeric_cols])\n",
    "\n",
    "# 1.4 Create target column for Prophet\n",
    "df['y'] = df['TOTAL_LOAD']\n",
    "\n",
    "# 2. Prophet decomposition\n",
    "prophet_df = df[['ds','y']]\n",
    "m = Prophet(yearly_seasonality=True, weekly_seasonality=True, daily_seasonality=True)\n",
    "m.fit(prophet_df)\n",
    "forecast = m.predict(prophet_df)\n",
    "df['trend']       = forecast['trend'].values\n",
    "df['seasonality'] = forecast['yearly'].values\n",
    "\n",
    "# 3. Feature engineering\n",
    "df['hour']        = df['ds'].dt.hour\n",
    "df['day_of_week'] = df['ds'].dt.dayofweek\n",
    "df['is_weekend']  = (df['day_of_week'] >= 5).astype(int)\n",
    "\n",
    "# 3.1 Load lags and rolling\n",
    "df['lag_1']      = df['y'].shift(1)\n",
    "df['lag_24']     = df['y'].shift(24)\n",
    "df['roll3_mean'] = df['y'].rolling(3).mean()\n",
    "df['roll3_std']  = df['y'].rolling(3).std()\n",
    "\n",
    "# 3.2 Weather feature prefixes (customize these)\n",
    "weather_prefixes = ['JEJU_','GOSAN_','SUNGSAN_','SEOGWIPO_']\n",
    "weather_feats = [c for c in df.columns if any(c.startswith(p) for p in weather_prefixes)]\n",
    "\n",
    "# 3.3 Create weather lags\n",
    "for feat in weather_feats:\n",
    "    df[f'{feat}_lag1']  = df[feat].shift(1)\n",
    "    df[f'{feat}_lag24'] = df[feat].shift(24)\n",
    "\n",
    "# 3.4 Drop rows with any NaNs from shifting/rolling\n",
    "df = df.dropna().reset_index(drop=True)\n",
    "\n",
    "# 4. Train/test split\n",
    "cutoff = pd.to_datetime('2019-01-01')\n",
    "train = df[df['ds'] < cutoff].copy()\n",
    "test  = df[df['ds'] >= cutoff].copy()\n",
    "\n",
    "# 5. Prepare feature lists\n",
    "base_feats    = ['trend','seasonality','hour','day_of_week','is_weekend',\n",
    "                 'lag_1','lag_24','roll3_mean','roll3_std']\n",
    "lagged_weather = [c for c in df.columns if c.endswith('_lag1') or c.endswith('_lag24')]\n",
    "full_feats     = base_feats + weather_feats + lagged_weather\n",
    "\n",
    "# 6. Hybrid Embedded + Stability Feature Selection\n",
    "# 6.1 Filter out constant features\n",
    "vt = VarianceThreshold(threshold=1e-8)\n",
    "vt.fit(train[full_feats])\n",
    "filtered_feats = [f for f,keep in zip(full_feats, vt.get_support()) if keep]\n",
    "\n",
    "# 6.2 Stability selection\n",
    "B, K, tau = 30, 15, 0.6\n",
    "counts = Counter()\n",
    "\n",
    "for b in range(B):\n",
    "    sample = train.sample(frac=1.0, replace=True, random_state=b)\n",
    "    dtrain_b = xgb.DMatrix(sample[filtered_feats], label=sample['y'])\n",
    "    model_b = xgb.train(\n",
    "        {'objective':'reg:squarederror','tree_method':'hist',\n",
    "         'max_depth':4,'learning_rate':0.1,'seed':b},\n",
    "        dtrain_b, num_boost_round=50, verbose_eval=False\n",
    "    )\n",
    "    imp = model_b.get_score(importance_type='gain')\n",
    "    topk = [f for f,_ in sorted(imp.items(), key=lambda x: x[1], reverse=True)[:K]]\n",
    "    counts.update(topk)\n",
    "\n",
    "selected_feats = [f for f,c in counts.items() if c >= tau * B]\n",
    "print(\"Selected features:\", selected_feats)\n",
    "\n",
    "# 7. Cross-Validation & Final Model Training\n",
    "dtrain = xgb.DMatrix(train[selected_feats], label=train['y'])\n",
    "dtest  = xgb.DMatrix(test[selected_feats],  label=test ['y'])\n",
    "\n",
    "params = {\n",
    "    'objective':'reg:squarederror','tree_method':'hist',\n",
    "    'learning_rate':0.05,'max_depth':6,\n",
    "    'subsample':0.8,'colsample_bytree':0.8,\n",
    "    'eval_metric':'mae','seed':42\n",
    "}\n",
    "cv = xgb.cv(\n",
    "    params, dtrain, num_boost_round=1000,\n",
    "    nfold=5, early_stopping_rounds=20,\n",
    "    metrics='mae', as_pandas=True, seed=42\n",
    ")\n",
    "best_n = len(cv)\n",
    "\n",
    "bst = xgb.train(params, dtrain, num_boost_round=best_n)\n",
    "\n",
    "# 8. Predict & Evaluate\n",
    "y_pred = bst.predict(dtest)\n",
    "y_true = test['y'].values\n",
    "\n",
    "\n",
    "mae  = mean_absolute_error(y_true, y_pred)\n",
    "mse  = mean_squared_error(y_true, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "mape = mean_absolute_percentage_error(y_true, y_pred) * 100\n",
    "\n",
    "print(f\"Test MAE:  {mae:.2f}\")\n",
    "print(f\"Test MSE:  {mse:.2f}\")\n",
    "print(f\"Test RMSE: {rmse:.2f}\")\n",
    "print(f\"Test MAPE: {mape:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175c748e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error\n",
    "\n",
    "# 1. Build results DataFrame\n",
    "df_res = test[['ds','y']].copy()\n",
    "df_res['pred'] = y_pred\n",
    "df_res['Day']   = df_res['ds'].dt.day_name()\n",
    "df_res['Week']  = df_res['ds'].dt.isocalendar().week\n",
    "df_res['Month'] = df_res['ds'].dt.month_name()\n",
    "\n",
    "# 2. Metric computation function\n",
    "def compute_metrics(g):\n",
    "    y_true, y_pr = g['y'], g['pred']\n",
    "    mae  = mean_absolute_error(y_true, y_pr)\n",
    "    mse  = mean_squared_error(y_true, y_pr)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mape = mean_absolute_percentage_error(y_true, y_pr)*100\n",
    "    return pd.Series({'MAE':mae,'MSE':mse,'RMSE':rmse,'MAPE (%)':mape})\n",
    "\n",
    "# 3. Compute tables\n",
    "daily   = df_res.groupby('Day').apply(compute_metrics).reindex(\n",
    "            ['Monday','Tuesday','Wednesday','Thursday','Friday','Saturday','Sunday'])\n",
    "\n",
    "monthly = df_res.groupby('Month').apply(compute_metrics).reindex([\n",
    "            'January','February','March','April','May','June',\n",
    "            'July','August','September','October','November','December'])\n",
    "\n",
    "# 4. Display\n",
    "print(\"### Daily Metrics\")\n",
    "print(daily.to_markdown())\n",
    "\n",
    "print(\"\\n### Monthly Metrics\")\n",
    "print(monthly.to_markdown())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4caa984d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "sns.set(style=\"whitegrid\", font_scale=1.0)\n",
    "\n",
    "# Recompute gain_full from your trained booster\n",
    "gain = bst.get_score(importance_type='gain')\n",
    "gain_full = {f: gain.get(f, 0.0) for f in selected_feats}\n",
    "\n",
    "# Prepare data\n",
    "rounds    = np.arange(len(cv))\n",
    "train_mu  = cv['train-mae-mean']\n",
    "valid_mu  = cv['test-mae-mean']\n",
    "residuals = y_true - y_pred\n",
    "\n",
    "# Top-10 selected features by gain\n",
    "fi = pd.Series(gain_full).sort_values().tail(10)\n",
    "\n",
    "# Create 2×2 subplot\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "(ax1, ax2), (ax3, ax4) = axes\n",
    "\n",
    "# A) Learning Curve\n",
    "ax1.plot(rounds, train_mu, label='Train MAE', lw=2)\n",
    "ax1.plot(rounds, valid_mu, label='Val MAE',   lw=2)\n",
    "ax1.axvline(best_n, color='gray', ls='--', label=f'Optimal ({best_n})')\n",
    "ax1.set(xlabel='Boosting Round', ylabel='MAE', title='Learning Curve')\n",
    "ax1.legend()\n",
    "ax1.text(-0.05, 1.05, 'A', transform=ax1.transAxes,\n",
    "         fontsize=16, fontweight='bold')\n",
    "\n",
    "# B) Actual vs. Predicted\n",
    "ax2.plot(test['ds'], y_true, label='Actual',   color='#2e7d32', lw=1)\n",
    "ax2.plot(test['ds'], y_pred, label='Predicted', color='#e64a19', lw=1)\n",
    "ax2.set(xlabel='Date', ylabel='Load', title='Actual vs. Predicted')\n",
    "ax2.legend()\n",
    "ax2.text(-0.05, 1.05, 'B', transform=ax2.transAxes,\n",
    "         fontsize=16, fontweight='bold')\n",
    "\n",
    "# C) Residual Distribution\n",
    "sns.histplot(residuals, kde=True, stat=\"density\", ax=ax3, color='#9ccc65')\n",
    "ax3.set(xlabel='Residual', title='Residual Distribution')\n",
    "ax3.text(-0.05, 1.05, 'C', transform=ax3.transAxes,\n",
    "         fontsize=16, fontweight='bold')\n",
    "\n",
    "# D) Top-10 Feature Importances\n",
    "features = fi.index.tolist()\n",
    "importances = fi.values\n",
    "cmap = plt.get_cmap('tab10')\n",
    "colors = [cmap(i % 10) for i in range(len(features))]\n",
    "ax4.barh(features, importances, color=colors)\n",
    "ax4.set(xlabel='Gain Importance', title='Top-10 Feature Importances')\n",
    "ax4.text(-0.05, 1.05, 'D', transform=ax4.transAxes,\n",
    "         fontsize=16, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639cd40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total features before selection: {len(full_feats)}\")\n",
    "print(f\"Features selected for training: {len(selected_feats)}\")\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "n_full     = len(full_feats)\n",
    "n_selected = len(selected_feats)\n",
    "\n",
    "plt.figure(figsize=(5,4))\n",
    "bars = plt.bar(\n",
    "    ['All features','Selected'],\n",
    "    [n_full, n_selected],\n",
    "    color=['#e28743','#1f77b4']\n",
    ")\n",
    "plt.ylabel('Feature Count')\n",
    "plt.title('Features Before vs. After Selection')\n",
    "for bar in bars:\n",
    "    h = bar.get_height()\n",
    "    plt.text(bar.get_x()+bar.get_width()/2, h+1, str(h),\n",
    "             ha='center', va='bottom', fontweight='bold')\n",
    "plt.ylim(0, n_full*1.1)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be7d5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1. Compute counts\n",
    "n_selected   = len(selected_feats)\n",
    "n_unselected = len(full_feats) - n_selected\n",
    "n_total      = len(full_feats)\n",
    "pct_selected   = n_selected   / n_total * 100\n",
    "pct_unselected = n_unselected / n_total * 100\n",
    "\n",
    "# 2. Set up figure with two subplots: horizontal bar + donut\n",
    "fig, (ax_bar, ax_donut) = plt.subplots(\n",
    "    1, 2, figsize=(14, 5),\n",
    "    gridspec_kw={'width_ratios': [2, 1]}\n",
    ")\n",
    "\n",
    "# 3. Horizontal Bar Chart (counts only)\n",
    "labels = ['Total Features', 'Unselected', 'Selected']\n",
    "counts = [n_total, n_unselected, n_selected]\n",
    "colors = ['#ff6f00', '#1f77b4', '#9ccc65']\n",
    "\n",
    "y_pos = range(len(labels))\n",
    "ax_bar.barh(y_pos, counts, color=colors, edgecolor='none')\n",
    "ax_bar.set_yticks(y_pos)\n",
    "ax_bar.set_yticklabels(labels, fontsize=12)\n",
    "ax_bar.invert_yaxis()  # highest at top\n",
    "ax_bar.set_xlabel('Count', fontsize=12)\n",
    "ax_bar.set_title('Feature Counts', fontsize=14, pad=10)\n",
    "ax_bar.grid(axis='x', linestyle='--', alpha=0.5)\n",
    "\n",
    "# Annotate counts (no percentages)\n",
    "for i, count in enumerate(counts):\n",
    "    ax_bar.text(\n",
    "        count + n_total * 0.01, i,\n",
    "        str(count),\n",
    "        va='center', fontsize=11, fontweight='bold'\n",
    "    )\n",
    "\n",
    "ax_bar.set_xlim(0, n_total * 1.1)\n",
    "\n",
    "# 4. Donut Chart for Selected vs Unselected\n",
    "sizes = [n_selected, n_unselected]\n",
    "d_colors = ['#9ccc65', '#9e9e9e']\n",
    "wedges, texts = ax_donut.pie(\n",
    "    sizes, labels=None, colors=d_colors,\n",
    "    startangle=90, wedgeprops={'width': 0.5, 'edgecolor': 'white'}\n",
    ")\n",
    "ax_donut.set_title('Selection Proportion', fontsize=14, pad=10)\n",
    "\n",
    "# Legend with percentages\n",
    "ax_donut.legend(\n",
    "    wedges,\n",
    "    [f'Selected ({pct_selected:.1f}%)', f'Unselected ({pct_unselected:.1f}%)'],\n",
    "    title='',\n",
    "    loc='center',\n",
    "    bbox_to_anchor=(0.5, -0.1),\n",
    "    ncol=1,\n",
    "    fontsize=12\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd404fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# 9. Plot Feature Importance with Different Colors\n",
    "# Get feature importance scores from the final model\n",
    "importance = bst.get_score(importance_type='gain')\n",
    "importance = {k: v for k, v in importance.items() if k in selected_feats}\n",
    "\n",
    "# Sort features by importance\n",
    "sorted_importance = dict(sorted(importance.items(), key=lambda x: x[1], reverse=True))\n",
    "features = list(sorted_importance.keys())\n",
    "scores = list(sorted_importance.values())\n",
    "\n",
    "# Generate a colormap for different colors\n",
    "colors = plt.cm.tab20(np.linspace(0, 1, len(features)))\n",
    "\n",
    "# Create bar plot with different colors for each bar\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(features, scores, color=colors)\n",
    "plt.xlabel('Feature Importance (Gain)')\n",
    "plt.title('Feature Importance for Selected Features')\n",
    "plt.gca().invert_yaxis()  # Highest importance at the top\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save and show the plot\n",
    "plt.savefig('feature_importance.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5594758",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.inspection import partial_dependence\n",
    "from xgboost import XGBRegressor, DMatrix\n",
    "from collections import defaultdict\n",
    "\n",
    "# 11. Overlaid Partial Dependence and Residual Plots for Common Weather Variables\n",
    "\n",
    "# Filter weather features that are in selected_feats\n",
    "weather_in_sel = [f for f in weather_feats if f in selected_feats]\n",
    "\n",
    "# Group weather features by common variable (e.g., DI, TEMP)\n",
    "# Assume feature format is {location}_{variable}, e.g., JEJU_DI\n",
    "weather_groups = defaultdict(list)\n",
    "prefixes = ['JEJU_', 'GOSAN_', 'SUNGSAN_', 'SEOGWIPO_']\n",
    "for feat in weather_in_sel:\n",
    "    for prefix in prefixes:\n",
    "        if feat.startswith(prefix):\n",
    "            variable = feat[len(prefix):]  # Extract variable (e.g., DI)\n",
    "            weather_groups[variable].append(feat)\n",
    "            break\n",
    "\n",
    "# Prepare test data for residual calculations\n",
    "df_plot = test.copy()\n",
    "df_plot['y_pred'] = bst.predict(DMatrix(test[selected_feats]))\n",
    "df_plot['residual'] = df_plot['y'] - df_plot['y_pred']\n",
    "\n",
    "# Create an XGBRegressor model equivalent to bst for partial dependence\n",
    "params = {\n",
    "    'objective': 'reg:squarederror',\n",
    "    'tree_method': 'hist',\n",
    "    'learning_rate': 0.05,\n",
    "    'max_depth': 6,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'eval_metric': 'mae',\n",
    "    'seed': 42\n",
    "}\n",
    "xgb_regressor = XGBRegressor(**params)\n",
    "xgb_regressor.fit(train[selected_feats], train['y'])\n",
    "\n",
    "# Create a single figure with subplots for each common variable\n",
    "n_variables = len(weather_groups)\n",
    "if n_variables > 0:\n",
    "    fig, axes = plt.subplots(n_variables, 2, figsize=(10, n_variables * 3), squeeze=False)\n",
    "\n",
    "    # Use tab10 colormap for distinct colors per location\n",
    "    colors = plt.cm.tab10(np.linspace(0, 1, len(prefixes)))\n",
    "    location_colors = {prefix: colors[i] for i, prefix in enumerate(prefixes)}\n",
    "\n",
    "    for i, (variable, features) in enumerate(weather_groups.items()):\n",
    "        # 11.1 Partial Dependence Plot (Left Column)\n",
    "        for feat in features:\n",
    "            feature_idx = selected_feats.index(feat)\n",
    "            pd_results = partial_dependence(\n",
    "                xgb_regressor,\n",
    "                X=train[selected_feats],\n",
    "                features=[feature_idx],\n",
    "                grid_resolution=50,\n",
    "                kind='average'\n",
    "            )\n",
    "            # Extract location prefix for labeling and coloring\n",
    "            location = next(prefix for prefix in prefixes if feat.startswith(prefix))\n",
    "            axes[i, 0].plot(\n",
    "                pd_results[\"grid_values\"][0],\n",
    "                pd_results[\"average\"][0],\n",
    "                label=location[:-1],  # Remove underscore for label\n",
    "                color=location_colors.get(location, 'black')\n",
    "            )\n",
    "        axes[i, 0].set_xlabel(variable)\n",
    "        axes[i, 0].set_ylabel('Partial Dependence')\n",
    "        axes[i, 0].set_title(f'Partial Dependence: {variable}')\n",
    "        axes[i, 0].legend()\n",
    "\n",
    "        # 11.2 Residual vs. Feature Scatter Plot (Right Column)\n",
    "        for feat in features:\n",
    "            location = next(prefix for prefix in prefixes if feat.startswith(prefix))\n",
    "            sns.scatterplot(\n",
    "                x=feat,\n",
    "                y='residual',\n",
    "                data=df_plot,\n",
    "                alpha=0.4,\n",
    "                s=10,\n",
    "                label=location[:-1],\n",
    "                color=location_colors.get(location, 'black'),\n",
    "                ax=axes[i, 1]\n",
    "            )\n",
    "        axes[i, 1].axhline(0, color='black', linestyle='--', linewidth=1)\n",
    "        axes[i, 1].set_xlabel(variable)\n",
    "        axes[i, 1].set_ylabel('Residual (Actual - Predicted)')\n",
    "        axes[i, 1].set_title(f'Residual vs. {variable}')\n",
    "        axes[i, 1].legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('overlaid_weather_plots.png')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No weather features found in selected_feats. Skipping overlaid plot.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6faee322",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b18bd77c",
   "metadata": {},
   "source": [
    "## Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764f2bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from prophet import Prophet\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    mean_absolute_error, mean_squared_error,\n",
    "    mean_absolute_percentage_error, r2_score\n",
    ")\n",
    "\n",
    "import xgboost as xgb\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "##############################################################################################################\n",
    "# GPU / DEVICE SETUP\n",
    "##############################################################################################################\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"\\n✓ GPU Detected: Using CUDA\")\n",
    "    print(\"CUDA Version:\", torch.version.cuda)\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"\\n× GPU NOT detected — using CPU\")\n",
    "\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "\n",
    "##############################################################################################################\n",
    "# UTILITIES FOR COMPLEXITY\n",
    "##############################################################################################################\n",
    "def get_model_size_mb(model, framework=\"torch\"):\n",
    "    \"\"\"\n",
    "    Save model to a temporary file and measure size in MB.\n",
    "    framework: \"torch\", \"xgb\", \"pickle\"\n",
    "    \"\"\"\n",
    "    if framework == \"torch\":\n",
    "        tmp_file = \"tmp_model.pth\"\n",
    "        torch.save(model.state_dict(), tmp_file)\n",
    "    elif framework == \"xgb\":\n",
    "        tmp_file = \"tmp_model.json\"\n",
    "        model.save_model(tmp_file)\n",
    "    else:  # generic pickle (Prophet, sklearn, statsmodels, etc.)\n",
    "        tmp_file = \"tmp_model.pkl\"\n",
    "        with open(tmp_file, \"wb\") as f:\n",
    "            pickle.dump(model, f)\n",
    "\n",
    "    size_mb = os.path.getsize(tmp_file) / (1024 * 1024)\n",
    "    os.remove(tmp_file)\n",
    "    return size_mb\n",
    "\n",
    "\n",
    "def measure_inference_time(model, X, framework=\"torch\"):\n",
    "    \"\"\"\n",
    "    Measure total and per-sample inference time.\n",
    "    Avoid tensor.numpy() to work around no-NumPy PyTorch builds.\n",
    "    \"\"\"\n",
    "    if framework == \"torch\":\n",
    "        model.eval()\n",
    "        t0 = time.time()\n",
    "        with torch.no_grad():\n",
    "            _ = model(torch.tensor(X).float().to(device)).cpu()  # no .numpy()\n",
    "        t1 = time.time()\n",
    "    elif framework == \"xgb\":\n",
    "        t0 = time.time()\n",
    "        _ = model.predict(xgb.DMatrix(X.astype(\"float32\")))\n",
    "        t1 = time.time()\n",
    "    elif framework == \"sk\":  # sklearn-like\n",
    "        t0 = time.time()\n",
    "        _ = model.predict(X)\n",
    "        t1 = time.time()\n",
    "    elif framework == \"prophet\":\n",
    "        t0 = time.time()\n",
    "        _ = model.predict(X)\n",
    "        t1 = time.time()\n",
    "    else:\n",
    "        t0 = time.time()\n",
    "        _ = model.predict(X)\n",
    "        t1 = time.time()\n",
    "\n",
    "    total = t1 - t0\n",
    "    per_sample = total / len(X)\n",
    "    return total, per_sample\n",
    "\n",
    "\n",
    "def gpu_memory_used_mb():\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()\n",
    "        used = torch.cuda.max_memory_allocated() / (1024 * 1024)\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "        return used\n",
    "    else:\n",
    "        return 0.0\n",
    "\n",
    "\n",
    "##############################################################################################################\n",
    "# STEP 1: Load + Preprocess\n",
    "##############################################################################################################\n",
    "df = pd.read_csv(\"totalload_new.csv\", engine=\"python\")\n",
    "df = df.dropna(axis=1, how=\"all\")\n",
    "\n",
    "df[\"ds\"] = pd.to_datetime(df[\"BASE_DT\"], format=\"%d-%m-%y %H:%M\", errors=\"coerce\")\n",
    "df = df.dropna(subset=[\"ds\"]).sort_values(\"ds\").reset_index(drop=True)\n",
    "\n",
    "numeric_cols = df.select_dtypes(include=[\"float64\", \"int64\"]).columns\n",
    "df[numeric_cols] = SimpleImputer(strategy=\"mean\").fit_transform(df[numeric_cols])\n",
    "\n",
    "df[\"y\"] = df[\"TOTAL_LOAD\"]\n",
    "\n",
    "\n",
    "##############################################################################################################\n",
    "# STEP 2: Prophet Decomposition (for features)\n",
    "##############################################################################################################\n",
    "m_decomp = Prophet(weekly_seasonality=True, yearly_seasonality=True, daily_seasonality=True)\n",
    "m_decomp.fit(df[[\"ds\", \"y\"]])\n",
    "fc = m_decomp.predict(df[[\"ds\"]])\n",
    "\n",
    "df[\"trend\"] = fc[\"trend\"].values\n",
    "df[\"seasonality\"] = fc[\"yearly\"].values\n",
    "\n",
    "\n",
    "##############################################################################################################\n",
    "# STEP 3: Feature Engineering\n",
    "##############################################################################################################\n",
    "df[\"hour\"] = df[\"ds\"].dt.hour\n",
    "df[\"dow\"] = df[\"ds\"].dt.dayofweek\n",
    "df[\"weekend\"] = (df[\"dow\"] >= 5).astype(int)\n",
    "\n",
    "df[\"lag1\"] = df[\"y\"].shift(1)\n",
    "df[\"lag24\"] = df[\"y\"].shift(24)\n",
    "df[\"roll3_mean\"] = df[\"y\"].rolling(3).mean()\n",
    "df[\"roll3_std\"] = df[\"y\"].rolling(3).std()\n",
    "\n",
    "weather_prefix = [\"JEJU_\", \"GOSAN_\", \"SUNGSAN_\", \"SEOGWIPO_\"]\n",
    "weather_feats = [c for c in df.columns if any(c.startswith(p) for p in weather_prefix)]\n",
    "\n",
    "for wf in weather_feats:\n",
    "    df[f\"{wf}_lag1\"] = df[wf].shift(1)\n",
    "    df[f\"{wf}_lag24\"] = df[wf].shift(24)\n",
    "\n",
    "df = df.dropna().reset_index(drop=True)\n",
    "\n",
    "base_feats = [\"trend\",\"seasonality\",\"hour\",\"dow\",\"weekend\",\n",
    "              \"lag1\",\"lag24\",\"roll3_mean\",\"roll3_std\"]\n",
    "lag_weather = [c for c in df.columns if c.endswith(\"_lag1\") or c.endswith(\"_lag24\")]\n",
    "full_feats = base_feats + weather_feats + lag_weather\n",
    "\n",
    "\n",
    "##############################################################################################################\n",
    "# STEP 4: Train/Test Split\n",
    "##############################################################################################################\n",
    "cut = pd.to_datetime(\"2019-01-01\")\n",
    "train = df[df[\"ds\"] < cut].copy()\n",
    "test  = df[df[\"ds\"] >= cut].copy()\n",
    "\n",
    "\n",
    "##############################################################################################################\n",
    "# STEP 5: Feature Selection (Variance + Stability) using CPU XGBoost (hist)\n",
    "##############################################################################################################\n",
    "vt = VarianceThreshold(1e-8)\n",
    "vt.fit(train[full_feats])\n",
    "filtered = [f for f, keep in zip(full_feats, vt.get_support()) if keep]\n",
    "\n",
    "B, K, tau = 30, 15, 0.6\n",
    "counts = Counter()\n",
    "\n",
    "for b in range(B):\n",
    "    samp = train.sample(frac=1.0, replace=True, random_state=b)\n",
    "    dmat = xgb.DMatrix(samp[filtered].astype(np.float32), label=samp[\"y\"].astype(np.float32))\n",
    "    model_b = xgb.train(\n",
    "        {\"objective\": \"reg:squarederror\", \"tree_method\": \"hist\"},\n",
    "        dmat, num_boost_round=50\n",
    "    )\n",
    "    imp = model_b.get_score(importance_type=\"gain\")\n",
    "    topk = sorted(imp.items(), key=lambda x: x[1], reverse=True)[:K]\n",
    "    counts.update([f for f,_ in topk])\n",
    "\n",
    "selected_feats = [f for f, c in counts.items() if c >= B * tau]\n",
    "#print(f\"\\nSelected Features: {len(selected_feats)}\")\n",
    "\n",
    "\n",
    "##############################################################################################################\n",
    "# STEP 6: Scaling + Sequence Preparation\n",
    "##############################################################################################################\n",
    "scX = StandardScaler()\n",
    "scY = StandardScaler()\n",
    "\n",
    "Xtr = scX.fit_transform(train[selected_feats])\n",
    "Xts = scX.transform(test[selected_feats])\n",
    "\n",
    "ytr = scY.fit_transform(train[\"y\"].values.reshape(-1,1)).ravel()\n",
    "yts = scY.transform(test[\"y\"].values.reshape(-1,1)).ravel()\n",
    "\n",
    "\n",
    "def create_sequences(X, y, L):\n",
    "    Xs, ys = [], []\n",
    "    for i in range(len(X)-L):\n",
    "        Xs.append(X[i:i+L])\n",
    "        ys.append(y[i+L])\n",
    "    return np.array(Xs), np.array(ys)\n",
    "\n",
    "\n",
    "LOOKBACK = 24\n",
    "Xtr_seq, ytr_seq = create_sequences(Xtr, ytr, LOOKBACK)\n",
    "Xts_seq, yts_seq = create_sequences(Xts, yts, LOOKBACK)\n",
    "\n",
    "\n",
    "##############################################################################################################\n",
    "# MODEL 1 — ProphetBoost (XGBoost, CPU hist)\n",
    "##############################################################################################################\n",
    "print(\"\\n=== ProphetBoost (XGBoost, hist) ===\")\n",
    "dtrain = xgb.DMatrix(Xtr.astype(\"float32\"), label=ytr.astype(\"float32\"))\n",
    "dtest  = xgb.DMatrix(Xts.astype(\"float32\"), label=yts.astype(\"float32\"))\n",
    "\n",
    "xgb_params = {\n",
    "    \"objective\": \"reg:squarederror\",\n",
    "    \"tree_method\": \"hist\",   # CPU histogram\n",
    "    \"learning_rate\": 0.05,\n",
    "    \"max_depth\": 6,\n",
    "    \"subsample\": 0.8,\n",
    "    \"colsample_bytree\": 0.8,\n",
    "    \"eval_metric\": \"mae\"\n",
    "}\n",
    "\n",
    "cv = xgb.cv(xgb_params, dtrain, 1000, nfold=5, early_stopping_rounds=20,\n",
    "            metrics=\"mae\", as_pandas=True)\n",
    "best_rounds = len(cv)\n",
    "\n",
    "t0 = time.time()\n",
    "xgb_model = xgb.train(xgb_params, dtrain, best_rounds)\n",
    "train_time_xgb = time.time() - t0\n",
    "\n",
    "infer_total_xgb, infer_per_xgb = measure_inference_time(\n",
    "    xgb_model, Xts, framework=\"xgb\"\n",
    ")\n",
    "size_xgb = get_model_size_mb(xgb_model, framework=\"xgb\")\n",
    "gpu_mem_xgb = gpu_memory_used_mb()\n",
    "\n",
    "pred_xgb_scaled = xgb_model.predict(dtest)\n",
    "pred_xgb = scY.inverse_transform(pred_xgb_scaled.reshape(-1,1)).ravel()\n",
    "\n",
    "\n",
    "##############################################################################################################\n",
    "# MODEL 2 — Transformer (VALID d_model % heads == 0)\n",
    "##############################################################################################################\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, num_layers, dim_ff, input_dim):\n",
    "        super().__init__()\n",
    "        self.pe = self._pe(LOOKBACK, d_model)\n",
    "        self.proj = nn.Linear(input_dim, d_model)\n",
    "\n",
    "        enc_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, nhead=num_heads,\n",
    "            dim_feedforward=dim_ff, batch_first=True\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(enc_layer, num_layers=num_layers)\n",
    "        self.fc = nn.Linear(d_model, 1)\n",
    "\n",
    "    def _pe(self, L, D):\n",
    "        pos = torch.arange(L).unsqueeze(1)\n",
    "        div = torch.exp(torch.arange(0, D, 2) * -(np.log(10000.0) / D))\n",
    "        pe = torch.zeros(1, L, D)\n",
    "        pe[0,:,0::2] = torch.sin(pos * div)\n",
    "        pe[0,:,1::2] = torch.cos(pos * div)\n",
    "        return pe\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.proj(x) + self.pe[:, :x.size(1), :].to(device)\n",
    "        x = self.encoder(x)\n",
    "        return self.fc(x[:,-1,:])\n",
    "\n",
    "\n",
    "def objective_tf(trial):\n",
    "\n",
    "    d_model = trial.suggest_categorical(\"d_model\", [32, 64, 128])\n",
    "    num_heads = trial.suggest_categorical(\"num_heads\", [1, 2, 4, 8])\n",
    "    num_layers = trial.suggest_int(\"num_layers\", 1, 3)\n",
    "    dim_ff = trial.suggest_categorical(\"dim_ff\", [64,128,256])\n",
    "    lr = trial.suggest_float(\"lr\", 1e-4, 1e-2, log=True)\n",
    "    batch = trial.suggest_categorical(\"batch\", [16,32,64])\n",
    "\n",
    "    if d_model % num_heads != 0:\n",
    "        return float(\"inf\")\n",
    "\n",
    "    model = TransformerModel(d_model, num_heads, num_layers, dim_ff,\n",
    "                             input_dim=Xtr_seq.shape[2]).to(device)\n",
    "\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    loss_fn = nn.MSELoss()\n",
    "\n",
    "    loader = DataLoader(TensorDataset(\n",
    "        torch.tensor(Xtr_seq).float(),\n",
    "        torch.tensor(ytr_seq).float().unsqueeze(-1)\n",
    "    ), batch_size=batch, shuffle=False)\n",
    "\n",
    "    model.train()\n",
    "    for _ in range(20):\n",
    "        for xb,yb in loader:\n",
    "            xb,yb = xb.to(device), yb.to(device)\n",
    "            opt.zero_grad()\n",
    "            loss = loss_fn(model(xb), yb)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        pred_t = model(torch.tensor(Xts_seq).float().to(device)).cpu().detach().view(-1)\n",
    "        pred = np.array(pred_t.tolist())\n",
    "        loss = mean_squared_error(yts_seq[:len(pred)], pred)\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "print(\"\\nSearching Transformer HPO...\")\n",
    "study_tf = optuna.create_study(direction=\"minimize\")\n",
    "study_tf.optimize(objective_tf, n_trials=10)\n",
    "best_tf = study_tf.best_params\n",
    "print(\"Transformer Best Params:\", best_tf)\n",
    "\n",
    "print(\"\\n=== Transformer Model ===\")\n",
    "model_tf = TransformerModel(\n",
    "    d_model=best_tf[\"d_model\"],\n",
    "    num_heads=best_tf[\"num_heads\"],\n",
    "    num_layers=best_tf[\"num_layers\"],\n",
    "    dim_ff=best_tf[\"dim_ff\"],\n",
    "    input_dim=Xtr_seq.shape[2]\n",
    ").to(device)\n",
    "\n",
    "opt_tf = torch.optim.Adam(model_tf.parameters(), lr=best_tf[\"lr\"])\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "loader_tf = DataLoader(TensorDataset(\n",
    "    torch.tensor(Xtr_seq).float(),\n",
    "    torch.tensor(ytr_seq).float().unsqueeze(-1)\n",
    "), batch_size=best_tf[\"batch\"], shuffle=False)\n",
    "\n",
    "t0 = time.time()\n",
    "model_tf.train()\n",
    "for _ in range(50):\n",
    "    for xb,yb in loader_tf:\n",
    "        xb,yb = xb.to(device), yb.to(device)\n",
    "        opt_tf.zero_grad()\n",
    "        loss = loss_fn(model_tf(xb), yb)\n",
    "        loss.backward()\n",
    "        opt_tf.step()\n",
    "train_time_tf = time.time() - t0\n",
    "\n",
    "infer_total_tf, infer_per_tf = measure_inference_time(\n",
    "    model_tf, Xts_seq, framework=\"torch\"\n",
    ")\n",
    "size_tf = get_model_size_mb(model_tf, framework=\"torch\")\n",
    "gpu_mem_tf = gpu_memory_used_mb()\n",
    "\n",
    "model_tf.eval()\n",
    "with torch.no_grad():\n",
    "    pred_tf_t = model_tf(torch.tensor(Xts_seq).float().to(device)).cpu().detach().view(-1)\n",
    "    pred_tf_scaled = np.array(pred_tf_t.tolist())\n",
    "pred_tf = scY.inverse_transform(pred_tf_scaled.reshape(-1,1)).ravel()\n",
    "\n",
    "\n",
    "##############################################################################################################\n",
    "# MODEL 3 — DLINEAR\n",
    "##############################################################################################################\n",
    "class DLinear(nn.Module):\n",
    "    def __init__(self, seq_len):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(seq_len, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x[:,:,0]\n",
    "        return self.fc(x)\n",
    "\n",
    "\n",
    "def objective_dl(trial):\n",
    "\n",
    "    lr = trial.suggest_float(\"lr\", 1e-4, 1e-2, log=True)\n",
    "    batch = trial.suggest_categorical(\"batch\", [16,32,64])\n",
    "\n",
    "    model = DLinear(LOOKBACK).to(device)\n",
    "    opti = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    loss_fn = nn.MSELoss()\n",
    "\n",
    "    loader = DataLoader(TensorDataset(\n",
    "        torch.tensor(Xtr_seq).float(),\n",
    "        torch.tensor(ytr_seq).float().unsqueeze(-1)\n",
    "    ), batch_size=batch, shuffle=False)\n",
    "\n",
    "    model.train()\n",
    "    for _ in range(20):\n",
    "        for xb,yb in loader:\n",
    "            xb,yb = xb.to(device), yb.to(device)\n",
    "            opti.zero_grad()\n",
    "            loss = loss_fn(model(xb), yb)\n",
    "            loss.backward()\n",
    "            opti.step()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        pred_t = model(torch.tensor(Xts_seq).float().to(device)).cpu().detach().view(-1)\n",
    "        pred = np.array(pred_t.tolist())\n",
    "        loss = mean_squared_error(yts_seq[:len(pred)], pred)\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "print(\"\\nSearching DLinear HPO...\")\n",
    "study_dl = optuna.create_study(direction=\"minimize\")\n",
    "study_dl.optimize(objective_dl, n_trials=10)\n",
    "best_dl = study_dl.best_params\n",
    "print(\"DLinear Best Params:\", best_dl)\n",
    "\n",
    "print(\"\\n=== DLinear ===\")\n",
    "model_dl = DLinear(LOOKBACK).to(device)\n",
    "opt_dl = torch.optim.Adam(model_dl.parameters(), lr=best_dl[\"lr\"])\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "loader_dl = DataLoader(TensorDataset(\n",
    "    torch.tensor(Xtr_seq).float(),\n",
    "    torch.tensor(ytr_seq).float().unsqueeze(-1)\n",
    "), batch_size=best_dl[\"batch\"], shuffle=False)\n",
    "\n",
    "t0 = time.time()\n",
    "model_dl.train()\n",
    "for _ in range(50):\n",
    "    for xb,yb in loader_dl:\n",
    "        xb,yb = xb.to(device), yb.to(device)\n",
    "        opt_dl.zero_grad()\n",
    "        loss = loss_fn(model_dl(xb), yb)\n",
    "        loss.backward()\n",
    "        opt_dl.step()\n",
    "train_time_dl = time.time() - t0\n",
    "\n",
    "infer_total_dl, infer_per_dl = measure_inference_time(\n",
    "    model_dl, Xts_seq, framework=\"torch\"\n",
    ")\n",
    "size_dl = get_model_size_mb(model_dl, framework=\"torch\")\n",
    "gpu_mem_dl = gpu_memory_used_mb()\n",
    "\n",
    "model_dl.eval()\n",
    "with torch.no_grad():\n",
    "    pred_dl_t = model_dl(torch.tensor(Xts_seq).float().to(device)).cpu().detach().view(-1)\n",
    "    pred_dl_scaled = np.array(pred_dl_t.tolist())\n",
    "pred_dl = scY.inverse_transform(pred_dl_scaled.reshape(-1,1)).ravel()\n",
    "\n",
    "\n",
    "##############################################################################################################\n",
    "# MODEL 4 — DEEP RVFL ENSEMBLE\n",
    "##############################################################################################################\n",
    "print(\"\\n=== Deep RVFL Ensemble ===\")\n",
    "\n",
    "class DeepRVFL(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim,256), nn.ReLU(),\n",
    "            nn.Linear(256,256), nn.ReLU(),\n",
    "            nn.Linear(256,1)\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = x.reshape(x.size(0),-1)\n",
    "        return self.net(x)\n",
    "\n",
    "rvfl_models = []\n",
    "for _ in range(5):\n",
    "    mdl = DeepRVFL(Xtr_seq.shape[1] * Xtr_seq.shape[2]).to(device)\n",
    "    opti = torch.optim.Adam(mdl.parameters(), lr=1e-3)\n",
    "    rvfl_models.append((mdl, opti))\n",
    "\n",
    "loader_rvfl = DataLoader(TensorDataset(\n",
    "    torch.tensor(Xtr_seq).float(),\n",
    "    torch.tensor(ytr_seq).float().unsqueeze(-1)\n",
    "), batch_size=32, shuffle=False)\n",
    "\n",
    "t0 = time.time()\n",
    "for mdl, opti in rvfl_models:\n",
    "    mdl.train()\n",
    "    for _ in range(20):\n",
    "        for xb,yb in loader_rvfl:\n",
    "            xb,yb = xb.to(device), yb.to(device)\n",
    "            opti.zero_grad()\n",
    "            loss = loss_fn(mdl(xb), yb)\n",
    "            loss.backward()\n",
    "            opti.step()\n",
    "train_time_rvfl = time.time() - t0\n",
    "\n",
    "t0 = time.time()\n",
    "preds_rvfl_scaled = []\n",
    "for mdl,_ in rvfl_models:\n",
    "    mdl.eval()\n",
    "    with torch.no_grad():\n",
    "        p_t = mdl(torch.tensor(Xts_seq).float().to(device)).cpu().detach().view(-1)\n",
    "        p = np.array(p_t.tolist())\n",
    "        preds_rvfl_scaled.append(p)\n",
    "infer_total_rvfl = time.time() - t0\n",
    "infer_per_rvfl = infer_total_rvfl / len(Xts_seq)\n",
    "\n",
    "pred_rvfl_scaled = np.mean(preds_rvfl_scaled, axis=0)\n",
    "pred_rvfl = scY.inverse_transform(pred_rvfl_scaled.reshape(-1,1)).ravel()\n",
    "\n",
    "size_rvfl = get_model_size_mb(rvfl_models[0][0], framework=\"torch\")\n",
    "gpu_mem_rvfl = gpu_memory_used_mb()\n",
    "\n",
    "\n",
    "##############################################################################################################\n",
    "# MODEL 5 — CNN-LSTM\n",
    "##############################################################################################################\n",
    "print(\"\\nTraining CNN-LSTM...\")\n",
    "\n",
    "class CNNLSTM(nn.Module):\n",
    "    \"\"\"\n",
    "    Conv1d over features (channels) + LSTM over time.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, conv_channels=64, hidden_dim=128, num_layers=1):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=input_dim,\n",
    "                               out_channels=conv_channels,\n",
    "                               kernel_size=3,\n",
    "                               padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.lstm = nn.LSTM(conv_channels, hidden_dim,\n",
    "                            num_layers=num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self.relu(self.conv1(x))      # (B, C, T)\n",
    "        x = x.permute(0, 2, 1)            # (B, T, C)\n",
    "        out, _ = self.lstm(x)             # (B, T, H)\n",
    "        return self.fc(out[:, -1, :])\n",
    "\n",
    "model_cnnlstm = CNNLSTM(input_dim=Xtr_seq.shape[2]).to(device)\n",
    "opt_cnnlstm = torch.optim.Adam(model_cnnlstm.parameters(), lr=1e-3)\n",
    "\n",
    "loader_cnnlstm = DataLoader(\n",
    "    TensorDataset(\n",
    "        torch.tensor(Xtr_seq).float(),\n",
    "        torch.tensor(ytr_seq).float().unsqueeze(-1)\n",
    "    ),\n",
    "    batch_size=64, shuffle=False\n",
    ")\n",
    "\n",
    "t0 = time.time()\n",
    "model_cnnlstm.train()\n",
    "for _ in range(50):\n",
    "    for xb,yb in loader_cnnlstm:\n",
    "        xb,yb = xb.to(device), yb.to(device)\n",
    "        opt_cnnlstm.zero_grad()\n",
    "        loss = loss_fn(model_cnnlstm(xb), yb)\n",
    "        loss.backward()\n",
    "        opt_cnnlstm.step()\n",
    "train_time_cnnlstm = time.time() - t0\n",
    "\n",
    "infer_total_cnnlstm, infer_per_cnnlstm = measure_inference_time(\n",
    "    model_cnnlstm, Xts_seq, framework=\"torch\"\n",
    ")\n",
    "size_cnnlstm = get_model_size_mb(model_cnnlstm, framework=\"torch\")\n",
    "gpu_mem_cnnlstm = gpu_memory_used_mb()\n",
    "\n",
    "model_cnnlstm.eval()\n",
    "with torch.no_grad():\n",
    "    pred_cnnlstm_t = model_cnnlstm(torch.tensor(Xts_seq).float().to(device)).cpu().detach().view(-1)\n",
    "    pred_cnnlstm_scaled = np.array(pred_cnnlstm_t.tolist())\n",
    "pred_cnnlstm = scY.inverse_transform(pred_cnnlstm_scaled.reshape(-1, 1)).ravel()\n",
    "\n",
    "\n",
    "##############################################################################################################\n",
    "# MODEL 6 — CNN-ANN\n",
    "##############################################################################################################\n",
    "print(\"\\nTraining CNN-ANN...\")\n",
    "\n",
    "class CNNANN(nn.Module):\n",
    "    \"\"\"\n",
    "    Conv1d over features + global pooling + MLP.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, conv_channels=64):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=input_dim,\n",
    "                               out_channels=conv_channels,\n",
    "                               kernel_size=3,\n",
    "                               padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(conv_channels, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 1)           # (B, F, T)\n",
    "        x = self.relu(self.conv1(x))     # (B, C, T)\n",
    "        x = self.pool(x).squeeze(-1)     # (B, C)\n",
    "        return self.mlp(x)               # (B, 1)\n",
    "\n",
    "model_cnnann = CNNANN(input_dim=Xtr_seq.shape[2]).to(device)\n",
    "opt_cnnann = torch.optim.Adam(model_cnnann.parameters(), lr=1e-3)\n",
    "\n",
    "loader_cnnann = DataLoader(\n",
    "    TensorDataset(\n",
    "        torch.tensor(Xtr_seq).float(),\n",
    "        torch.tensor(ytr_seq).float().unsqueeze(-1)\n",
    "    ),\n",
    "    batch_size=64, shuffle=False\n",
    ")\n",
    "\n",
    "t0 = time.time()\n",
    "model_cnnann.train()\n",
    "for _ in range(50):\n",
    "    for xb,yb in loader_cnnann:\n",
    "        xb,yb = xb.to(device), yb.to(device)\n",
    "        opt_cnnann.zero_grad()\n",
    "        loss = loss_fn(model_cnnann(xb), yb)\n",
    "        loss.backward()\n",
    "        opt_cnnann.step()\n",
    "train_time_cnnann = time.time() - t0\n",
    "\n",
    "infer_total_cnnann, infer_per_cnnann = measure_inference_time(\n",
    "    model_cnnann, Xts_seq, framework=\"torch\"\n",
    ")\n",
    "size_cnnann = get_model_size_mb(model_cnnann, framework=\"torch\")\n",
    "gpu_mem_cnnann = gpu_memory_used_mb()\n",
    "\n",
    "model_cnnann.eval()\n",
    "with torch.no_grad():\n",
    "    pred_cnnann_t = model_cnnann(torch.tensor(Xts_seq).float().to(device)).cpu().detach().view(-1)\n",
    "    pred_cnnann_scaled = np.array(pred_cnnann_t.tolist())\n",
    "pred_cnnann = scY.inverse_transform(pred_cnnann_scaled.reshape(-1, 1)).ravel()\n",
    "\n",
    "\n",
    "##############################################################################################################\n",
    "# MODEL 7 — ARIMA-LSTM (ARIMA + residual LSTM)\n",
    "##############################################################################################################\n",
    "print(\"\\nTraining ARIMA-LSTM (ARIMA + residual LSTM)...\")\n",
    "\n",
    "t0 = time.time()\n",
    "y_train_raw = train[\"y\"].values\n",
    "y_test_raw  = test[\"y\"].values\n",
    "\n",
    "arima_order = (5, 1, 0)\n",
    "arima_model = ARIMA(y_train_raw, order=arima_order)\n",
    "arima_result = arima_model.fit()\n",
    "\n",
    "arima_pred_train = arima_result.predict(start=0, end=len(y_train_raw)-1)\n",
    "arima_pred_test  = arima_result.predict(start=len(y_train_raw),\n",
    "                                        end=len(y_train_raw)+len(y_test_raw)-1)\n",
    "\n",
    "residual_train = y_train_raw - arima_pred_train\n",
    "scRes = StandardScaler()\n",
    "res_tr_scaled = scRes.fit_transform(residual_train.reshape(-1, 1)).ravel()\n",
    "\n",
    "Xtr_res_seq, ytr_res_seq = create_sequences(Xtr, res_tr_scaled, LOOKBACK)\n",
    "\n",
    "class ResidualLSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=64, num_layers=1):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim,\n",
    "                            num_layers=num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        return self.fc(out[:, -1, :])\n",
    "\n",
    "model_res_lstm = ResidualLSTM(input_dim=Xtr_res_seq.shape[2]).to(device)\n",
    "opt_res = torch.optim.Adam(model_res_lstm.parameters(), lr=1e-3)\n",
    "\n",
    "loader_res = DataLoader(\n",
    "    TensorDataset(\n",
    "        torch.tensor(Xtr_res_seq).float(),\n",
    "        torch.tensor(ytr_res_seq).float().unsqueeze(-1)\n",
    "    ),\n",
    "    batch_size=64, shuffle=False\n",
    ")\n",
    "\n",
    "model_res_lstm.train()\n",
    "for _ in range(50):\n",
    "    for xb, yb in loader_res:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        opt_res.zero_grad()\n",
    "        loss = loss_fn(model_res_lstm(xb), yb)\n",
    "        loss.backward()\n",
    "        opt_res.step()\n",
    "train_time_arima_lstm = time.time() - t0\n",
    "\n",
    "# Inference: ARIMA test + LSTM residual\n",
    "t0 = time.time()\n",
    "arima_test_seq = arima_pred_test[LOOKBACK:]\n",
    "with torch.no_grad():\n",
    "    res_pred_t = model_res_lstm(\n",
    "        torch.tensor(Xts_seq).float().to(device)\n",
    "    ).cpu().detach().view(-1)\n",
    "    res_pred = np.array(res_pred_t.tolist())\n",
    "pred_arima_lstm = arima_test_seq + res_pred\n",
    "infer_total_arima_lstm = time.time() - t0\n",
    "infer_per_arima_lstm = infer_total_arima_lstm / len(Xts_seq)\n",
    "\n",
    "size_arima = get_model_size_mb(arima_result, framework=\"pickle\")\n",
    "size_res_lstm = get_model_size_mb(model_res_lstm, framework=\"torch\")\n",
    "size_arima_lstm = size_arima + size_res_lstm\n",
    "gpu_mem_arima_lstm = gpu_memory_used_mb()\n",
    "\n",
    "\n",
    "##############################################################################################################\n",
    "# FINAL METRICS \n",
    "##############################################################################################################\n",
    "def metrics(ytrue, ypred):\n",
    "    mae = mean_absolute_error(ytrue, ypred)\n",
    "    mse = mean_squared_error(ytrue, ypred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mape = mean_absolute_percentage_error(ytrue, ypred)*100\n",
    "    r2 = r2_score(ytrue, ypred)\n",
    "    return mae, mse, rmse, mape, r2\n",
    "\n",
    "# Align all sequence-based models to same horizon (RVFL length)\n",
    "y_true_final = test[\"y\"].values[-len(pred_rvfl):]\n",
    "\n",
    "pb_pred         = pred_xgb[-len(y_true_final):]\n",
    "tf_pred         = pred_tf[-len(y_true_final):]\n",
    "dl_pred         = pred_dl[-len(y_true_final):]\n",
    "rvfl_pred       = pred_rvfl\n",
    "cnnlstm_pred    = pred_cnnlstm[-len(y_true_final):]\n",
    "cnnann_pred     = pred_cnnann[-len(y_true_final):]\n",
    "arima_lstm_pred = pred_arima_lstm[-len(y_true_final):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef1f92d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n================== FINAL RESULTS ==================\")\n",
    "print(\"Model              MAE        MSE        RMSE       MAPE      R2\")\n",
    "print(\"--------------------------------------------------------------------------\")\n",
    "print(f\"ProphetBoost   {metrics(y_true_final, pb_pred)}\")\n",
    "print(f\"Transformer    {metrics(y_true_final, tf_pred)}\")\n",
    "print(f\"DLinear        {metrics(y_true_final, dl_pred)}\")\n",
    "print(f\"Deep RVFL      {metrics(y_true_final, rvfl_pred)}\")\n",
    "print(f\"CNN-LSTM       {metrics(y_true_final, cnnlstm_pred)}\")\n",
    "print(f\"CNN-ANN        {metrics(y_true_final, cnnann_pred)}\")\n",
    "print(f\"ARIMA-LSTM     {metrics(y_true_final, arima_lstm_pred)}\")\n",
    "print(\"==========================================================================\\n\")\n",
    "\n",
    "\n",
    "##############################################################################################################\n",
    "# MODEL COMPLEXITY SUMMARY\n",
    "##############################################################################################################\n",
    "print(\"\\n================ MODEL COMPLEXITY ==================\")\n",
    "print(\"Model          | Train(s) | Infer(s) | Infer/sample(ms) | Size(MB) | GPU Mem(MB)\")\n",
    "print(\"---------------------------------------------------------------------------------\")\n",
    "print(f\"ProphetBoost   | {train_time_xgb:8.3f}     | {infer_total_xgb:8.3f}     | {infer_per_xgb*1000:12.4f}     | {size_xgb:8.2f}     | {gpu_mem_xgb:10.2f}\")\n",
    "print(f\"Transformer    | {train_time_tf:8.3f}      | {infer_total_tf:8.3f}      | {infer_per_tf*1000:12.4f}      | {size_tf:8.2f}      | {gpu_mem_tf:10.2f}\")\n",
    "print(f\"DLinear        | {train_time_dl:8.3f}      | {infer_total_dl:8.3f}      | {infer_per_dl*1000:12.4f}      | {size_dl:8.2f}      | {gpu_mem_dl:10.2f}\")\n",
    "print(f\"Deep RVFL      | {train_time_rvfl:8.3f}    | {infer_total_rvfl:8.3f}    | {infer_per_rvfl*1000:12.4f}    | {size_rvfl:8.2f}    | {gpu_mem_rvfl:10.2f}\")\n",
    "print(f\"CNN-LSTM       | {train_time_cnnlstm:8.3f} | {infer_total_cnnlstm:8.3f} | {infer_per_cnnlstm*1000:12.4f} | {size_cnnlstm:8.2f} | {gpu_mem_cnnlstm:10.2f}\")\n",
    "print(f\"CNN-ANN        | {train_time_cnnann:8.3f}  | {infer_total_cnnann:8.3f}  | {infer_per_cnnann*1000:12.4f}  | {size_cnnann:8.2f}  | {gpu_mem_cnnann:10.2f}\")\n",
    "print(f\"ARIMA-LSTM     | {train_time_arima_lstm:8.3f} | {infer_total_arima_lstm:8.3f} | {infer_per_arima_lstm*1000:12.4f} | {size_arima_lstm:8.2f} | {gpu_mem_arima_lstm:10.2f}\")\n",
    "print(\"---------------------------------------------------------------------------------\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4027a56f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "267944f3",
   "metadata": {},
   "source": [
    "## Statistical Table (Wilcoxon + Friedman)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03f89e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from scipy.stats import wilcoxon, friedmanchisquare\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 1. Construct error dataframe\n",
    "# -------------------------------------------------------\n",
    "results_df = pd.DataFrame({\n",
    "    \"y_true\"      : y_true_final,\n",
    "    \"ProphetBoost\": pb_pred[-len(y_true_final):],\n",
    "    \"Transformer\" : tf_pred[-len(y_true_final):],\n",
    "    \"DLinear\"     : dl_pred[-len(y_true_final):],\n",
    "    \"RVFL\"        : rvfl_pred[-len(y_true_final):],\n",
    "    \"CNN-LSTM\"    : cnnlstm_pred[-len(y_true_final):],\n",
    "    \"CNN-ANN\"     : cnnann_pred[-len(y_true_final):],\n",
    "    \"ARIMA-LSTM\"  : arima_lstm_pred[-len(y_true_final):]\n",
    "})\n",
    "\n",
    "error_df = pd.DataFrame({\n",
    "    model: abs(results_df[\"y_true\"] - results_df[model])\n",
    "    for model in results_df.columns if model != \"y_true\"\n",
    "})\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 2. Wilcoxon Table Construction\n",
    "# -------------------------------------------------------\n",
    "baseline = \"ProphetBoost\"\n",
    "rows = []\n",
    "\n",
    "for model in error_df.columns:\n",
    "    if model == baseline:\n",
    "        continue\n",
    "\n",
    "    W, p = wilcoxon(error_df[baseline], error_df[model])\n",
    "\n",
    "    # Significance markers\n",
    "    sig_025 = \"a\" if p < 0.025 else \"\"\n",
    "    sig_05  = \"a\" if p < 0.05  else \"\"\n",
    "    star_025 = \"**\" if p < 0.025 else \"*\"\n",
    "    star_05  = \"**\" if p < 0.05 else \"*\"\n",
    "\n",
    "    rows.append({\n",
    "        \"Compared Model\": f\"{baseline} vs {model}\",\n",
    "        \"W (α=0.025)\": f\"{int(W)} {sig_025}\",\n",
    "        \"p-value (α=0.025)\": f\"{p:.5f} {star_025}\",\n",
    "        \"W (α=0.05)\": f\"{int(W)} {sig_05}\",\n",
    "        \"p-value (α=0.05)\": f\"{p:.5f} {star_05}\"\n",
    "    })\n",
    "\n",
    "wilcoxon_table = pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 3. Friedman Test\n",
    "# -------------------------------------------------------\n",
    "friedman_stat, friedman_p = friedmanchisquare(\n",
    "    *[error_df[m] for m in error_df.columns]\n",
    ")\n",
    "\n",
    "friedman_result = {\n",
    "    \"Friedman Statistic\": f\"{friedman_stat:.4f}\",\n",
    "    \"p-value\": f\"{friedman_p:.6f}\",\n",
    "    \"Decision (α=0.05)\": \"Reject H₀\" if friedman_p < 0.05 else \"Fail to Reject H₀\"\n",
    "}\n",
    "\n",
    "friedman_df = pd.DataFrame([friedman_result])\n",
    "\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 4. Print Results\n",
    "# -------------------------------------------------------\n",
    "print(\"\\n======================= WILCOXON TABLE =======================\")\n",
    "print(wilcoxon_table.to_string(index=False))\n",
    "\n",
    "print(\"\\n======================= FRIEDMAN TEST =======================\")\n",
    "print(friedman_df.to_string(index=False))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c94e9cb",
   "metadata": {},
   "source": [
    "## Without the feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9501c9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from prophet import Prophet\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    mean_absolute_error, mean_squared_error,\n",
    "    mean_absolute_percentage_error, r2_score\n",
    ")\n",
    "\n",
    "import xgboost as xgb\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "##############################################################################################################\n",
    "# GPU / DEVICE SETUP\n",
    "##############################################################################################################\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"\\n✓ GPU Detected: Using CUDA\")\n",
    "    print(\"CUDA Version:\", torch.version.cuda)\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"\\n× GPU NOT detected — using CPU\")\n",
    "\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "##############################################################################################################\n",
    "# UTILITIES FOR COMPLEXITY\n",
    "##############################################################################################################\n",
    "def get_model_size_mb(model, framework=\"torch\"):\n",
    "    \"\"\"\n",
    "    Save model to a temporary file and measure size in MB.\n",
    "    framework: \"torch\", \"xgb\", \"pickle\"\n",
    "    \"\"\"\n",
    "    if framework == \"torch\":\n",
    "        tmp_file = \"tmp_model.pth\"\n",
    "        torch.save(model.state_dict(), tmp_file)\n",
    "    elif framework == \"xgb\":\n",
    "        tmp_file = \"tmp_model.json\"\n",
    "        model.save_model(tmp_file)\n",
    "    else:  # generic pickle (Prophet, sklearn, statsmodels, etc.)\n",
    "        tmp_file = \"tmp_model.pkl\"\n",
    "        with open(tmp_file, \"wb\") as f:\n",
    "            pickle.dump(model, f)\n",
    "\n",
    "    size_mb = os.path.getsize(tmp_file) / (1024 * 1024)\n",
    "    os.remove(tmp_file)\n",
    "    return size_mb\n",
    "\n",
    "\n",
    "def measure_inference_time(model, X, framework=\"torch\"):\n",
    "    \"\"\"\n",
    "    Measure total and per-sample inference time.\n",
    "    Avoid tensor.numpy() to work around no-NumPy PyTorch builds.\n",
    "    \"\"\"\n",
    "    if framework == \"torch\":\n",
    "        model.eval()\n",
    "        t0 = time.time()\n",
    "        with torch.no_grad():\n",
    "            _ = model(torch.tensor(X).float().to(device)).cpu()  # no .numpy()\n",
    "        t1 = time.time()\n",
    "    elif framework == \"xgb\":\n",
    "        t0 = time.time()\n",
    "        _ = model.predict(xgb.DMatrix(X.astype(\"float32\")))\n",
    "        t1 = time.time()\n",
    "    elif framework == \"sk\":  # sklearn-like\n",
    "        t0 = time.time()\n",
    "        _ = model.predict(X)\n",
    "        t1 = time.time()\n",
    "    elif framework == \"prophet\":\n",
    "        t0 = time.time()\n",
    "        _ = model.predict(X)\n",
    "        t1 = time.time()\n",
    "    else:\n",
    "        t0 = time.time()\n",
    "        _ = model.predict(X)\n",
    "        t1 = time.time()\n",
    "\n",
    "    total = t1 - t0\n",
    "    per_sample = total / len(X)\n",
    "    return total, per_sample\n",
    "\n",
    "\n",
    "def gpu_memory_used_mb():\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()\n",
    "        used = torch.cuda.max_memory_allocated() / (1024 * 1024)\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "        return used\n",
    "    else:\n",
    "        return 0.0\n",
    "\n",
    "##############################################################################################################\n",
    "# STEP 1: Load + Preprocess\n",
    "##############################################################################################################\n",
    "df = pd.read_csv(\"totalload_new.csv\", engine=\"python\")\n",
    "df = df.dropna(axis=1, how=\"all\")\n",
    "\n",
    "df[\"ds\"] = pd.to_datetime(df[\"BASE_DT\"], format=\"%d-%m-%y %H:%M\", errors=\"coerce\")\n",
    "df = df.dropna(subset=[\"ds\"]).sort_values(\"ds\").reset_index(drop=True)\n",
    "\n",
    "numeric_cols = df.select_dtypes(include=[\"float64\", \"int64\"]).columns\n",
    "df[numeric_cols] = SimpleImputer(strategy=\"mean\").fit_transform(df[numeric_cols])\n",
    "\n",
    "df[\"y\"] = df[\"TOTAL_LOAD\"]\n",
    "\n",
    "##############################################################################################################\n",
    "# STEP 2: Prophet Decomposition (for features, not as a baseline)\n",
    "##############################################################################################################\n",
    "m = Prophet(weekly_seasonality=True, yearly_seasonality=True, daily_seasonality=True)\n",
    "m.fit(df[[\"ds\", \"y\"]])\n",
    "fc = m.predict(df[[\"ds\"]])\n",
    "\n",
    "df[\"trend\"] = fc[\"trend\"].values\n",
    "df[\"seasonality\"] = fc[\"yearly\"].values\n",
    "\n",
    "##############################################################################################################\n",
    "# STEP 3: Feature Engineering\n",
    "##############################################################################################################\n",
    "df[\"hour\"] = df[\"ds\"].dt.hour\n",
    "df[\"dow\"] = df[\"ds\"].dt.dayofweek\n",
    "df[\"weekend\"] = (df[\"dow\"] >= 5).astype(int)\n",
    "\n",
    "df[\"lag1\"] = df[\"y\"].shift(1)\n",
    "df[\"lag24\"] = df[\"y\"].shift(24)\n",
    "df[\"roll3_mean\"] = df[\"y\"].rolling(3).mean()\n",
    "df[\"roll3_std\"] = df[\"y\"].rolling(3).std()\n",
    "\n",
    "weather_prefix = [\"JEJU_\", \"GOSAN_\", \"SUNGSAN_\", \"SEOGWIPO_\"]\n",
    "weather_feats = [c for c in df.columns if any(c.startswith(p) for p in weather_prefix)]\n",
    "\n",
    "for wf in weather_feats:\n",
    "    df[f\"{wf}_lag1\"] = df[wf].shift(1)\n",
    "    df[f\"{wf}_lag24\"] = df[wf].shift(24)\n",
    "\n",
    "df = df.dropna().reset_index(drop=True)\n",
    "\n",
    "base_feats = [\n",
    "    \"trend\", \"seasonality\", \"hour\", \"dow\", \"weekend\",\n",
    "    \"lag1\", \"lag24\", \"roll3_mean\", \"roll3_std\"\n",
    "]\n",
    "lag_weather = [c for c in df.columns if c.endswith(\"_lag1\") or c.endswith(\"_lag24\")]\n",
    "full_feats = base_feats + weather_feats + lag_weather\n",
    "\n",
    "##############################################################################################################\n",
    "# STEP 4: Train/Test Split\n",
    "##############################################################################################################\n",
    "cut = pd.to_datetime(\"2019-01-01\")\n",
    "train = df[df[\"ds\"] < cut].copy()\n",
    "test  = df[df[\"ds\"] >= cut].copy()\n",
    "\n",
    "##############################################################################################################\n",
    "# STEP 5: USE ALL FEATURES (NO SELECTION)\n",
    "##############################################################################################################\n",
    "selected_feats = full_feats\n",
    "print(f\"\\nUsing all features without selection: {len(selected_feats)} features\")\n",
    "\n",
    "##############################################################################################################\n",
    "# STEP 6: Scaling + Sequence Preparation\n",
    "##############################################################################################################\n",
    "scX = StandardScaler()\n",
    "scY = StandardScaler()\n",
    "\n",
    "Xtr = scX.fit_transform(train[selected_feats])\n",
    "Xts = scX.transform(test[selected_feats])\n",
    "\n",
    "ytr = scY.fit_transform(train[\"y\"].values.reshape(-1, 1)).ravel()\n",
    "yts = scY.transform(test[\"y\"].values.reshape(-1, 1)).ravel()\n",
    "\n",
    "def create_sequences(X, y, L):\n",
    "    Xs, ys = [], []\n",
    "    for i in range(len(X) - L):\n",
    "        Xs.append(X[i:i + L])\n",
    "        ys.append(y[i + L])\n",
    "    return np.array(Xs), np.array(ys)\n",
    "\n",
    "LOOKBACK = 24\n",
    "Xtr_seq, ytr_seq = create_sequences(Xtr, ytr, LOOKBACK)\n",
    "Xts_seq, yts_seq = create_sequences(Xts, yts, LOOKBACK)\n",
    "\n",
    "##############################################################################################################\n",
    "# MODEL 1 — ProphetBoost (XGBoost CPU hist)\n",
    "##############################################################################################################\n",
    "print(\"\\n=== ProphetBoost (XGBoost, hist) ===\")\n",
    "dtrain = xgb.DMatrix(Xtr.astype(\"float32\"), label=ytr.astype(\"float32\"))\n",
    "dtest  = xgb.DMatrix(Xts.astype(\"float32\"), label=yts.astype(\"float32\"))\n",
    "\n",
    "xgb_params = {\n",
    "    \"objective\": \"reg:squarederror\",\n",
    "    \"tree_method\": \"hist\",      # CPU-safe\n",
    "    \"learning_rate\": 0.05,\n",
    "    \"max_depth\": 6,\n",
    "    \"subsample\": 0.8,\n",
    "    \"colsample_bytree\": 0.8,\n",
    "    \"eval_metric\": \"mae\"\n",
    "}\n",
    "\n",
    "cv = xgb.cv(\n",
    "    xgb_params, dtrain, 1000,\n",
    "    nfold=5, early_stopping_rounds=20,\n",
    "    metrics=\"mae\", as_pandas=True\n",
    ")\n",
    "\n",
    "best_rounds = len(cv)\n",
    "\n",
    "t0 = time.time()\n",
    "xgb_model = xgb.train(xgb_params, dtrain, best_rounds)\n",
    "train_time_xgb = time.time() - t0\n",
    "\n",
    "infer_total_xgb, infer_per_xgb = measure_inference_time(\n",
    "    xgb_model, Xts, framework=\"xgb\"\n",
    ")\n",
    "size_xgb = get_model_size_mb(xgb_model, framework=\"xgb\")\n",
    "gpu_mem_xgb = gpu_memory_used_mb()\n",
    "\n",
    "pred_xgb_scaled = xgb_model.predict(dtest)\n",
    "pred_xgb = scY.inverse_transform(pred_xgb_scaled.reshape(-1, 1)).ravel()\n",
    "\n",
    "##############################################################################################################\n",
    "# MODEL 2 — Transformer (VALID d_model % heads == 0)\n",
    "##############################################################################################################\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, num_layers, dim_ff, input_dim):\n",
    "        super().__init__()\n",
    "        self.pe = self._pe(LOOKBACK, d_model)\n",
    "        self.proj = nn.Linear(input_dim, d_model)\n",
    "\n",
    "        enc_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, nhead=num_heads,\n",
    "            dim_feedforward=dim_ff, batch_first=True\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(enc_layer, num_layers=num_layers)\n",
    "        self.fc = nn.Linear(d_model, 1)\n",
    "\n",
    "    def _pe(self, L, D):\n",
    "        pos = torch.arange(L).unsqueeze(1)\n",
    "        div = torch.exp(torch.arange(0, D, 2) * -(np.log(10000.0) / D))\n",
    "        pe = torch.zeros(1, L, D)\n",
    "        pe[0, :, 0::2] = torch.sin(pos * div)\n",
    "        pe[0, :, 1::2] = torch.cos(pos * div)\n",
    "        return pe\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.proj(x) + self.pe[:, :x.size(1), :].to(device)\n",
    "        x = self.encoder(x)\n",
    "        return self.fc(x[:, -1, :])\n",
    "\n",
    "##############################################################################################################\n",
    "# Optuna Search Space (SAFE)\n",
    "##############################################################################################################\n",
    "def objective_tf(trial):\n",
    "\n",
    "    d_model = trial.suggest_categorical(\"d_model\", [32, 64, 128])\n",
    "    num_heads = trial.suggest_categorical(\"num_heads\", [1, 2, 4, 8])\n",
    "    num_layers = trial.suggest_int(\"num_layers\", 1, 3)\n",
    "    dim_ff = trial.suggest_categorical(\"dim_ff\", [64, 128, 256])\n",
    "    lr = trial.suggest_float(\"lr\", 1e-4, 1e-2, log=True)\n",
    "    batch = trial.suggest_categorical(\"batch\", [16, 32, 64])\n",
    "\n",
    "    if d_model % num_heads != 0:\n",
    "        return float(\"inf\")\n",
    "\n",
    "    model = TransformerModel(\n",
    "        d_model, num_heads, num_layers, dim_ff,\n",
    "        input_dim=Xtr_seq.shape[2]\n",
    "    ).to(device)\n",
    "\n",
    "    opt_ = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    loss_fn = nn.MSELoss()\n",
    "\n",
    "    loader = DataLoader(\n",
    "        TensorDataset(\n",
    "            torch.tensor(Xtr_seq).float(),\n",
    "            torch.tensor(ytr_seq).float().unsqueeze(-1)\n",
    "        ),\n",
    "        batch_size=batch,\n",
    "        shuffle=False   # avoid internal torch.randperm(...).numpy()\n",
    "    )\n",
    "\n",
    "    model.train()\n",
    "    for _ in range(20):\n",
    "        for xb, yb in loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            opt_.zero_grad()\n",
    "            loss = loss_fn(model(xb), yb)\n",
    "            loss.backward()\n",
    "            opt_.step()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        pred_t = model(torch.tensor(Xts_seq).float().to(device)).cpu().detach().view(-1)\n",
    "        pred = np.array(pred_t.tolist())\n",
    "        loss = mean_squared_error(yts_seq[:len(pred)], pred)\n",
    "\n",
    "    return loss\n",
    "\n",
    "print(\"\\nSearching Transformer HPO...\")\n",
    "study_tf = optuna.create_study(direction=\"minimize\")\n",
    "study_tf.optimize(objective_tf, n_trials=10)\n",
    "best_tf = study_tf.best_params\n",
    "print(\"Transformer Best Params:\", best_tf)\n",
    "\n",
    "##############################################################################################################\n",
    "# TRAIN FINAL TRANSFORMER\n",
    "##############################################################################################################\n",
    "model_tf = TransformerModel(\n",
    "    d_model=best_tf[\"d_model\"],\n",
    "    num_heads=best_tf[\"num_heads\"],\n",
    "    num_layers=best_tf[\"num_layers\"],\n",
    "    dim_ff=best_tf[\"dim_ff\"],\n",
    "    input_dim=Xtr_seq.shape[2]\n",
    ").to(device)\n",
    "\n",
    "opt_tf = torch.optim.Adam(model_tf.parameters(), lr=best_tf[\"lr\"])\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "loader_tf = DataLoader(\n",
    "    TensorDataset(\n",
    "        torch.tensor(Xtr_seq).float(),\n",
    "        torch.tensor(ytr_seq).float().unsqueeze(-1)\n",
    "    ),\n",
    "    batch_size=best_tf[\"batch\"],\n",
    "    shuffle=False   # avoid RandomSampler here too\n",
    ")\n",
    "\n",
    "t0 = time.time()\n",
    "model_tf.train()\n",
    "for _ in range(50):\n",
    "    for xb, yb in loader_tf:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        opt_tf.zero_grad()\n",
    "        loss = loss_fn(model_tf(xb), yb)\n",
    "        loss.backward()\n",
    "        opt_tf.step()\n",
    "train_time_tf = time.time() - t0\n",
    "\n",
    "infer_total_tf, infer_per_tf = measure_inference_time(\n",
    "    model_tf, Xts_seq, framework=\"torch\"\n",
    ")\n",
    "size_tf = get_model_size_mb(model_tf, framework=\"torch\")\n",
    "gpu_mem_tf = gpu_memory_used_mb()\n",
    "\n",
    "model_tf.eval()\n",
    "with torch.no_grad():\n",
    "    pred_tf_t = model_tf(torch.tensor(Xts_seq).float().to(device)).cpu().detach().view(-1)\n",
    "    pred_tf_scaled = np.array(pred_tf_t.tolist())\n",
    "pred_tf = scY.inverse_transform(pred_tf_scaled.reshape(-1, 1)).ravel()\n",
    "\n",
    "##############################################################################################################\n",
    "# MODEL 3 — DLINEAR\n",
    "##############################################################################################################\n",
    "class DLinear(nn.Module):\n",
    "    def __init__(self, seq_len):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(seq_len, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x[:, :, 0]\n",
    "        return self.fc(x)\n",
    "\n",
    "def objective_dl(trial):\n",
    "\n",
    "    lr = trial.suggest_float(\"lr\", 1e-4, 1e-2, log=True)\n",
    "    batch = trial.suggest_categorical(\"batch\", [16, 32, 64])\n",
    "\n",
    "    model = DLinear(LOOKBACK).to(device)\n",
    "    opti = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    loss_fn = nn.MSELoss()\n",
    "\n",
    "    loader = DataLoader(\n",
    "        TensorDataset(\n",
    "            torch.tensor(Xtr_seq).float(),\n",
    "            torch.tensor(ytr_seq).float().unsqueeze(-1)\n",
    "        ),\n",
    "        batch_size=batch,\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    model.train()\n",
    "    for _ in range(20):\n",
    "        for xb, yb in loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            opti.zero_grad()\n",
    "            loss = loss_fn(model(xb), yb)\n",
    "            loss.backward()\n",
    "            opti.step()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        pred_t = model(torch.tensor(Xts_seq).float().to(device)).cpu().detach().view(-1)\n",
    "        pred = np.array(pred_t.tolist())\n",
    "        loss = mean_squared_error(yts_seq[:len(pred)], pred)\n",
    "\n",
    "    return loss\n",
    "\n",
    "print(\"\\nSearching DLinear HPO...\")\n",
    "study_dl = optuna.create_study(direction=\"minimize\")\n",
    "study_dl.optimize(objective_dl, n_trials=10)\n",
    "best_dl = study_dl.best_params\n",
    "print(\"DLinear Best Params:\", best_dl)\n",
    "\n",
    "##############################################################################################################\n",
    "# TRAIN FINAL DLINEAR\n",
    "##############################################################################################################\n",
    "model_dl = DLinear(LOOKBACK).to(device)\n",
    "opt_dl = torch.optim.Adam(model_dl.parameters(), lr=best_dl[\"lr\"])\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "loader_dl = DataLoader(\n",
    "    TensorDataset(\n",
    "        torch.tensor(Xtr_seq).float(),\n",
    "        torch.tensor(ytr_seq).float().unsqueeze(-1)\n",
    "    ),\n",
    "    batch_size=best_dl[\"batch\"],\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "t0 = time.time()\n",
    "model_dl.train()\n",
    "for _ in range(50):\n",
    "    for xb, yb in loader_dl:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        opt_dl.zero_grad()\n",
    "        loss = loss_fn(model_dl(xb), yb)\n",
    "        loss.backward()\n",
    "        opt_dl.step()\n",
    "train_time_dl = time.time() - t0\n",
    "\n",
    "infer_total_dl, infer_per_dl = measure_inference_time(\n",
    "    model_dl, Xts_seq, framework=\"torch\"\n",
    ")\n",
    "size_dl = get_model_size_mb(model_dl, framework=\"torch\")\n",
    "gpu_mem_dl = gpu_memory_used_mb()\n",
    "\n",
    "model_dl.eval()\n",
    "with torch.no_grad():\n",
    "    pred_dl_t = model_dl(torch.tensor(Xts_seq).float().to(device)).cpu().detach().view(-1)\n",
    "    pred_dl_scaled = np.array(pred_dl_t.tolist())\n",
    "pred_dl = scY.inverse_transform(pred_dl_scaled.reshape(-1, 1)).ravel()\n",
    "\n",
    "##############################################################################################################\n",
    "# MODEL 4 — DEEP RVFL ENSEMBLE\n",
    "##############################################################################################################\n",
    "class DeepRVFL(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256), nn.ReLU(),\n",
    "            nn.Linear(256, 256), nn.ReLU(),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        return self.net(x)\n",
    "\n",
    "print(\"\\nTraining Deep RVFL Ensemble...\")\n",
    "rvfl_models = []\n",
    "for _ in range(5):\n",
    "    mdl = DeepRVFL(Xtr_seq.shape[1] * Xtr_seq.shape[2]).to(device)\n",
    "    opti = torch.optim.Adam(mdl.parameters(), lr=1e-3)\n",
    "    rvfl_models.append((mdl, opti))\n",
    "\n",
    "loader_rvfl = DataLoader(\n",
    "    TensorDataset(\n",
    "        torch.tensor(Xtr_seq).float(),\n",
    "        torch.tensor(ytr_seq).float().unsqueeze(-1)\n",
    "    ),\n",
    "    batch_size=32,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "t0 = time.time()\n",
    "for mdl, opti in rvfl_models:\n",
    "    mdl.train()\n",
    "    for _ in range(20):\n",
    "        for xb, yb in loader_rvfl:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            opti.zero_grad()\n",
    "            loss = loss_fn(mdl(xb), yb)\n",
    "            loss.backward()\n",
    "            opti.step()\n",
    "train_time_rvfl = time.time() - t0\n",
    "\n",
    "t0 = time.time()\n",
    "preds_rvfl_scaled = []\n",
    "for mdl, _ in rvfl_models:\n",
    "    mdl.eval()\n",
    "    with torch.no_grad():\n",
    "        p_t = mdl(torch.tensor(Xts_seq).float().to(device)).cpu().detach().view(-1)\n",
    "        p = np.array(p_t.tolist())\n",
    "        preds_rvfl_scaled.append(p)\n",
    "infer_total_rvfl = time.time() - t0\n",
    "infer_per_rvfl = infer_total_rvfl / len(Xts_seq)\n",
    "\n",
    "pred_rvfl_scaled = np.mean(preds_rvfl_scaled, axis=0)\n",
    "pred_rvfl = scY.inverse_transform(pred_rvfl_scaled.reshape(-1, 1)).ravel()\n",
    "\n",
    "size_rvfl = get_model_size_mb(rvfl_models[0][0], framework=\"torch\")\n",
    "gpu_mem_rvfl = gpu_memory_used_mb()\n",
    "\n",
    "##############################################################################################################\n",
    "# MODEL 5 — CNN-LSTM\n",
    "##############################################################################################################\n",
    "class CNNLSTM(nn.Module):\n",
    "    \"\"\"\n",
    "    Conv1d over features (channels) + LSTM over time.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, conv_channels=64, hidden_dim=128, num_layers=1):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=input_dim,\n",
    "                               out_channels=conv_channels,\n",
    "                               kernel_size=3,\n",
    "                               padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.lstm = nn.LSTM(conv_channels, hidden_dim,\n",
    "                            num_layers=num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self.relu(self.conv1(x))      # (B, C, T)\n",
    "        x = x.permute(0, 2, 1)            # (B, T, C)\n",
    "        out, _ = self.lstm(x)             # (B, T, H)\n",
    "        return self.fc(out[:, -1, :])\n",
    "\n",
    "print(\"\\nTraining CNN-LSTM...\")\n",
    "model_cnnlstm = CNNLSTM(input_dim=Xtr_seq.shape[2]).to(device)\n",
    "opt_cnnlstm = torch.optim.Adam(model_cnnlstm.parameters(), lr=1e-3)\n",
    "\n",
    "loader_cnnlstm = DataLoader(\n",
    "    TensorDataset(\n",
    "        torch.tensor(Xtr_seq).float(),\n",
    "        torch.tensor(ytr_seq).float().unsqueeze(-1)\n",
    "    ),\n",
    "    batch_size=64,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "t0 = time.time()\n",
    "model_cnnlstm.train()\n",
    "for _ in range(50):\n",
    "    for xb, yb in loader_cnnlstm:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        opt_cnnlstm.zero_grad()\n",
    "        loss = loss_fn(model_cnnlstm(xb), yb)\n",
    "        loss.backward()\n",
    "        opt_cnnlstm.step()\n",
    "train_time_cnnlstm = time.time() - t0\n",
    "\n",
    "infer_total_cnnlstm, infer_per_cnnlstm = measure_inference_time(\n",
    "    model_cnnlstm, Xts_seq, framework=\"torch\"\n",
    ")\n",
    "size_cnnlstm = get_model_size_mb(model_cnnlstm, framework=\"torch\")\n",
    "gpu_mem_cnnlstm = gpu_memory_used_mb()\n",
    "\n",
    "model_cnnlstm.eval()\n",
    "with torch.no_grad():\n",
    "    pred_cnnlstm_t = model_cnnlstm(torch.tensor(Xts_seq).float().to(device)).cpu().detach().view(-1)\n",
    "    pred_cnnlstm_scaled = np.array(pred_cnnlstm_t.tolist())\n",
    "pred_cnnlstm = scY.inverse_transform(pred_cnnlstm_scaled.reshape(-1, 1)).ravel()\n",
    "\n",
    "##############################################################################################################\n",
    "# MODEL 6 — CNN-ANN\n",
    "##############################################################################################################\n",
    "class CNNANN(nn.Module):\n",
    "    \"\"\"\n",
    "    Conv1d over features + global pooling + MLP.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, conv_channels=64):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=input_dim,\n",
    "                               out_channels=conv_channels,\n",
    "                               kernel_size=3,\n",
    "                               padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(conv_channels, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 1)           # (B, F, T)\n",
    "        x = self.relu(self.conv1(x))     # (B, C, T)\n",
    "        x = self.pool(x).squeeze(-1)     # (B, C)\n",
    "        return self.mlp(x)               # (B, 1)\n",
    "\n",
    "print(\"\\nTraining CNN-ANN...\")\n",
    "model_cnnann = CNNANN(input_dim=Xtr_seq.shape[2]).to(device)\n",
    "opt_cnnann = torch.optim.Adam(model_cnnann.parameters(), lr=1e-3)\n",
    "\n",
    "loader_cnnann = DataLoader(\n",
    "    TensorDataset(\n",
    "        torch.tensor(Xtr_seq).float(),\n",
    "        torch.tensor(ytr_seq).float().unsqueeze(-1)\n",
    "    ),\n",
    "    batch_size=64,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "t0 = time.time()\n",
    "model_cnnann.train()\n",
    "for _ in range(50):\n",
    "    for xb, yb in loader_cnnann:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        opt_cnnann.zero_grad()\n",
    "        loss = loss_fn(model_cnnann(xb), yb)\n",
    "        loss.backward()\n",
    "        opt_cnnann.step()\n",
    "train_time_cnnann = time.time() - t0\n",
    "\n",
    "infer_total_cnnann, infer_per_cnnann = measure_inference_time(\n",
    "    model_cnnann, Xts_seq, framework=\"torch\"\n",
    ")\n",
    "size_cnnann = get_model_size_mb(model_cnnann, framework=\"torch\")\n",
    "gpu_mem_cnnann = gpu_memory_used_mb()\n",
    "\n",
    "model_cnnann.eval()\n",
    "with torch.no_grad():\n",
    "    pred_cnnann_t = model_cnnann(torch.tensor(Xts_seq).float().to(device)).cpu().detach().view(-1)\n",
    "    pred_cnnann_scaled = np.array(pred_cnnann_t.tolist())\n",
    "pred_cnnann = scY.inverse_transform(pred_cnnann_scaled.reshape(-1, 1)).ravel()\n",
    "\n",
    "##############################################################################################################\n",
    "# MODEL 7 — ARIMA-LSTM (ARIMA + residual LSTM)\n",
    "##############################################################################################################\n",
    "print(\"\\nTraining ARIMA-LSTM (ARIMA + residual LSTM)...\")\n",
    "y_train_raw = train[\"y\"].values\n",
    "y_test_raw  = test[\"y\"].values\n",
    "\n",
    "arima_order = (5, 1, 0)\n",
    "arima_model = ARIMA(y_train_raw, order=arima_order)\n",
    "\n",
    "t0 = time.time()\n",
    "arima_result = arima_model.fit()\n",
    "train_time_arima = time.time() - t0\n",
    "\n",
    "arima_pred_train = arima_result.predict(start=0, end=len(y_train_raw)-1)\n",
    "arima_pred_test  = arima_result.predict(start=len(y_train_raw),\n",
    "                                        end=len(y_train_raw)+len(y_test_raw)-1)\n",
    "\n",
    "residual_train = y_train_raw - arima_pred_train\n",
    "scRes = StandardScaler()\n",
    "res_tr_scaled = scRes.fit_transform(residual_train.reshape(-1, 1)).ravel()\n",
    "\n",
    "def create_sequences_res(X, y, L):\n",
    "    Xs, ys = [], []\n",
    "    for i in range(len(X) - L):\n",
    "        Xs.append(X[i:i + L])\n",
    "        ys.append(y[i + L])\n",
    "    return np.array(Xs), np.array(ys)\n",
    "\n",
    "Xtr_res_seq, ytr_res_seq = create_sequences_res(Xtr, res_tr_scaled, LOOKBACK)\n",
    "\n",
    "class ResidualLSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=64, num_layers=1):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim,\n",
    "                            num_layers=num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        return self.fc(out[:, -1, :])\n",
    "\n",
    "print(\"\\nTraining residual LSTM...\")\n",
    "model_res_lstm = ResidualLSTM(input_dim=Xtr_res_seq.shape[2]).to(device)\n",
    "opt_res = torch.optim.Adam(model_res_lstm.parameters(), lr=1e-3)\n",
    "\n",
    "loader_res = DataLoader(\n",
    "    TensorDataset(\n",
    "        torch.tensor(Xtr_res_seq).float(),\n",
    "        torch.tensor(ytr_res_seq).float().unsqueeze(-1)\n",
    "    ),\n",
    "    batch_size=64,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "t0 = time.time()\n",
    "model_res_lstm.train()\n",
    "for _ in range(50):\n",
    "    for xb, yb in loader_res:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        opt_res.zero_grad()\n",
    "        loss = loss_fn(model_res_lstm(xb), yb)\n",
    "        loss.backward()\n",
    "        opt_res.step()\n",
    "train_time_res_lstm = time.time() - t0\n",
    "train_time_arima_lstm = train_time_arima + train_time_res_lstm\n",
    "\n",
    "# Inference: ARIMA test + LSTM residual\n",
    "t0 = time.time()\n",
    "arima_test_seq = arima_pred_test[LOOKBACK:]\n",
    "with torch.no_grad():\n",
    "    res_pred_t = model_res_lstm(\n",
    "        torch.tensor(Xts_seq).float().to(device)\n",
    "    ).cpu().detach().view(-1)\n",
    "    res_pred_scaled = np.array(res_pred_t.tolist())\n",
    "\n",
    "res_pred = scRes.inverse_transform(res_pred_scaled.reshape(-1, 1)).ravel()\n",
    "pred_arima_lstm = arima_test_seq + res_pred\n",
    "infer_total_arima_lstm = time.time() - t0\n",
    "infer_per_arima_lstm = infer_total_arima_lstm / len(Xts_seq)\n",
    "\n",
    "size_arima = get_model_size_mb(arima_result, framework=\"pickle\")\n",
    "size_res_lstm = get_model_size_mb(model_res_lstm, framework=\"torch\")\n",
    "size_arima_lstm = size_arima + size_res_lstm\n",
    "gpu_mem_arima_lstm = gpu_memory_used_mb()\n",
    "\n",
    "##############################################################################################################\n",
    "# FINAL METRICS \n",
    "##############################################################################################################\n",
    "def metrics(ytrue, ypred):\n",
    "    mae = mean_absolute_error(ytrue, ypred)\n",
    "    mse = mean_squared_error(ytrue, ypred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mape = mean_absolute_percentage_error(ytrue, ypred) * 100\n",
    "    r2 = r2_score(ytrue, ypred)\n",
    "    return mae, mse, rmse, mape, r2\n",
    "\n",
    "# Align all sequence-based models to same horizon (RVFL length)\n",
    "y_true_final = test[\"y\"].values[-len(pred_rvfl):]\n",
    "\n",
    "pb_pred         = pred_xgb[-len(y_true_final):]\n",
    "tf_pred         = pred_tf[-len(y_true_final):]\n",
    "dl_pred         = pred_dl[-len(y_true_final):]\n",
    "rvfl_pred       = pred_rvfl\n",
    "cnnlstm_pred    = pred_cnnlstm[-len(y_true_final):]\n",
    "cnnann_pred     = pred_cnnann[-len(y_true_final):]\n",
    "arima_lstm_pred = pred_arima_lstm[-len(y_true_final):]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a643d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n================== FINAL RESULTS ==================\")\n",
    "print(\"Model              MAE        MSE        RMSE       MAPE      R2\")\n",
    "print(\"--------------------------------------------------------------------------\")\n",
    "print(f\"ProphetBoost   {metrics(y_true_final, pb_pred)}\")\n",
    "print(f\"Transformer    {metrics(y_true_final, tf_pred)}\")\n",
    "print(f\"DLinear        {metrics(y_true_final, dl_pred)}\")\n",
    "print(f\"Deep RVFL      {metrics(y_true_final, rvfl_pred)}\")\n",
    "print(f\"CNN-LSTM       {metrics(y_true_final, cnnlstm_pred)}\")\n",
    "print(f\"CNN-ANN        {metrics(y_true_final, cnnann_pred)}\")\n",
    "print(f\"ARIMA-LSTM     {metrics(y_true_final, arima_lstm_pred)}\")\n",
    "print(\"==========================================================================\\n\")\n",
    "\n",
    "##############################################################################################################\n",
    "# MODEL COMPLEXITY SUMMARY\n",
    "##############################################################################################################\n",
    "print(\"\\n================ MODEL COMPLEXITY ==================\")\n",
    "print(\"Model          | Train(s) | Infer(s) | Infer/sample(ms) | Size(MB) | GPU Mem(MB)\")\n",
    "print(\"---------------------------------------------------------------------------------\")\n",
    "print(f\"ProphetBoost   | {train_time_xgb:8.3f} | {infer_total_xgb:8.3f} | {infer_per_xgb*1000:12.4f} | {size_xgb:8.2f} | {gpu_mem_xgb:10.2f}\")\n",
    "print(f\"Transformer    | {train_time_tf:8.3f} | {infer_total_tf:8.3f} | {infer_per_tf*1000:12.4f} | {size_tf:8.2f} | {gpu_mem_tf:10.2f}\")\n",
    "print(f\"DLinear        | {train_time_dl:8.3f} | {infer_total_dl:8.3f} | {infer_per_dl*1000:12.4f} | {size_dl:8.2f} | {gpu_mem_dl:10.2f}\")\n",
    "print(f\"Deep RVFL      | {train_time_rvfl:8.3f} | {infer_total_rvfl:8.3f} | {infer_per_rvfl*1000:12.4f} | {size_rvfl:8.2f} | {gpu_mem_rvfl:10.2f}\")\n",
    "print(f\"CNN-LSTM       | {train_time_cnnlstm:8.3f} | {infer_total_cnnlstm:8.3f} | {infer_per_cnnlstm*1000:12.4f} | {size_cnnlstm:8.2f} | {gpu_mem_cnnlstm:10.2f}\")\n",
    "print(f\"CNN-ANN        | {train_time_cnnann:8.3f} | {infer_total_cnnann:8.3f} | {infer_per_cnnann*1000:12.4f} | {size_cnnann:8.2f} | {gpu_mem_cnnann:10.2f}\")\n",
    "print(f\"ARIMA-LSTM     | {train_time_arima_lstm:8.3f} | {infer_total_arima_lstm:8.3f} | {infer_per_arima_lstm*1000:12.4f} | {size_arima_lstm:8.2f} | {gpu_mem_arima_lstm:10.2f}\")\n",
    "print(\"---------------------------------------------------------------------------------\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7336776",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mylab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
